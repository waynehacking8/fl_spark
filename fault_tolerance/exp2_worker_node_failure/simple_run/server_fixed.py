#!/usr/bin/env python
# -*- coding: utf-8 -*-
import socket
import threading
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import pickle
import struct
import os
import logging
from torchvision import datasets, transforms

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - FixedServer - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

HEADER_SIZE = 8

# Global variables
global_model = None
client_updates = []
processed_participants_count = 0
connected_participants_count = 0
connected_participant_ids = set()  # ğŸ”¥ æ–°å¢ï¼šè¿½è¹¤å·²é€£æ¥çš„åƒèˆ‡è€…ID
round_event = threading.Event()
updates_lock = threading.Lock()
processed_participants_lock = threading.Lock()
connection_lock = threading.Lock()

# Results directory and files
RESULTS_DIR = "../results/traditional/checkpoints"
results_file = os.path.join(RESULTS_DIR, "results.csv")
accuracy_history_file = os.path.join(RESULTS_DIR, "traditional_fl_accuracy.csv")

# GPU optimization
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    torch.cuda.set_device(0)
    torch.cuda.empty_cache()
logging.info(f"FixedServer using device: {device}")

def recv_all(sock, n):
    data = bytearray()
    while len(data) < n:
        try:
            packet = sock.recv(n - len(data))
            if not packet:
                return None
            data.extend(packet)
        except Exception as e:
            logging.error(f"recv_all error: {e}")
            return None
    return data

def recv_msg(sock, header_size=HEADER_SIZE):
    try:
        raw_msglen = recv_all(sock, header_size)
        if not raw_msglen:
            return None
        msglen = struct.unpack('>Q', raw_msglen)[0]
        raw_msg = recv_all(sock, msglen)
        if raw_msg is None:
            return None
        return pickle.loads(raw_msg)
    except Exception as e:
        logging.error(f"recv_msg error: {e}")
        return None

def send_msg(sock, data, header_size=HEADER_SIZE):
    try:
        msg = pickle.dumps(data)
        msg = struct.pack('>Q', len(msg)) + msg
        sock.sendall(msg)
        return True
    except Exception as e:
        logging.error(f"send_msg error: {e}")
        return False

class CNNMnist(nn.Module):
    def __init__(self):
        super(CNNMnist, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = torch.relu(x)
        x = self.conv2(x)
        x = torch.relu(x)
        x = torch.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return torch.log_softmax(x, dim=1)

def load_data(data_dir='./data'):
    """åŠ è¼‰æ¸¬è©¦æ•¸æ“š"""
    try:
        test_data = torch.load(os.path.join(data_dir, 'mnist_test.pt'), weights_only=False)
        logging.info(f"Loading MNIST test data from {data_dir}")
        
        if not os.path.exists(os.path.join(data_dir, 'MNIST')):
            logging.info("MNIST dataset already exists, skipping download")
        
        # ä½¿ç”¨èˆ‡originalç›¸åŒçš„æ¨™æº–åŒ–åƒæ•¸
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        # å‰µå»ºDataLoaderç”¨æ–¼æ¸¬è©¦
        data = test_data['data'].unsqueeze(1).float() / 255.0
        mean = 0.1307
        std = 0.3081
        data = (data - mean) / std
        targets = test_data['targets']
        
        test_dataset = torch.utils.data.TensorDataset(data, targets)
        test_loader = DataLoader(
            test_dataset,
            batch_size=64,
            shuffle=False,
            num_workers=0,
            pin_memory=False
        )
        
        return test_loader
    except Exception as e:
        logging.error(f"Error loading data: {e}")
        raise

def evaluate_model(model, test_loader, device):
    """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
    logging.info(f"Evaluating model on {device}...")
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += nn.functional.nll_loss(output, target, reduction='sum').item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
    
    test_loss /= total
    accuracy = 100. * correct / total
    
    logging.info(f'Evaluation complete. Test Loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.2f}%)')
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return accuracy, test_loss

# ç¢ºä¿çµæœç›®éŒ„å­˜åœ¨
os.makedirs(RESULTS_DIR, exist_ok=True)

# åˆå§‹åŒ–çµæœæ–‡ä»¶ï¼ˆåªåœ¨é–‹å§‹æ™‚æ¸…ç†ä¸€æ¬¡ï¼‰
if os.path.exists(results_file):
    os.remove(results_file)
    logging.info(f"æ¸…ç†èˆŠçš„çµæœæ–‡ä»¶: {results_file}")

if os.path.exists(accuracy_history_file):
    os.remove(accuracy_history_file)
    logging.info(f"æ¸…ç†èˆŠçš„æº–ç¢ºç‡æ–‡ä»¶: {accuracy_history_file}")

with open(results_file, 'w') as f:
    f.write('Round,Timestamp,Accuracy,Loss\n')

with open(accuracy_history_file, 'w') as f:
    f.write('Round,Accuracy\n')

logging.info("çµæœæ–‡ä»¶å·²åˆå§‹åŒ–")

def get_expected_participants(round_num):
    """æ ¹æ“šå¯¦é©—è¨­è¨ˆè¿”å›æœŸæœ›åƒèˆ‡è€…æ•¸é‡"""
    # æ‰€æœ‰è¼ªæ¬¡éƒ½æœŸæœ›4å€‹åƒèˆ‡è€…ï¼Œç¬¬8è¼ªé€šéæ•…éšœåµæ¸¬æ©Ÿåˆ¶è™•ç†
    return 4

def handle_client(client_socket, client_address, expected_round):
    """è™•ç†å®¢æˆ¶ç«¯é€£æ¥ï¼ŒåŠ å…¥è¼ªæ¬¡é©—è­‰æ©Ÿåˆ¶"""
    global client_updates, processed_participants_count, connected_participants_count, connected_participant_ids
    participant_id = f"{client_address[0]}:{client_address[1]}"
    logging.info(f"é–‹å§‹è™•ç†ä¾†è‡ª {participant_id} çš„å®¢æˆ¶ç«¯")

    try:
        client_socket.settimeout(180.0)

        # ğŸ”¥ è¼ªæ¬¡é©—è­‰ï¼šé¦–å…ˆæ¥æ”¶åƒèˆ‡è€…çš„è¼ªæ¬¡ä¿¡æ¯
        logging.info(f"ç­‰å¾… {participant_id} ç™¼é€è¼ªæ¬¡é©—è­‰ä¿¡æ¯...")
        round_info = recv_msg(client_socket)
        
        if round_info is None or not isinstance(round_info, dict) or 'round_num' not in round_info:
            logging.warning(f"âŒ {participant_id} æœªç™¼é€æœ‰æ•ˆè¼ªæ¬¡ä¿¡æ¯ï¼Œæ‹’çµ•é€£æ¥")
            client_socket.close()
            return

        participant_round = round_info['round_num']
        participant_id_num = round_info.get('participant_id', 'unknown')
        
        # é©—è­‰è¼ªæ¬¡åŒ¹é…
        if participant_round != expected_round:
            logging.warning(f"âŒ è¼ªæ¬¡ä¸åŒ¹é…ï¼š{participant_id} (åƒèˆ‡è€…{participant_id_num}) ç™¼é€ç¬¬{participant_round}è¼ªï¼Œæœå‹™å™¨æœŸæœ›ç¬¬{expected_round}è¼ªï¼Œæ‹’çµ•é€£æ¥")
            # ğŸ”§ ç™¼é€æ‹’çµ•ä¿¡æ¯ï¼ŒåŒ…å«æœå‹™å™¨ç•¶å‰æœŸæœ›çš„è¼ªæ¬¡
            send_msg(client_socket, {
                'status': 'rejected', 
                'reason': 'round_mismatch', 
                'expected': expected_round, 
                'received': participant_round,
                'server_current_round': expected_round,  # ğŸ”¥ æ–°å¢ï¼šæœå‹™å™¨ç•¶å‰è¼ªæ¬¡
                'sync_required': True  # ğŸ”¥ æ–°å¢ï¼šéœ€è¦åŒæ­¥æ¨™èªŒ
            })
            client_socket.close()
            return
        
        # ğŸ”¥ æª¢æŸ¥åƒèˆ‡è€…æ˜¯å¦å·²é€£æ¥ï¼ˆå»é‡æª¢æŸ¥ï¼‰
        with connection_lock:
            if participant_id_num in connected_participant_ids:
                logging.warning(f"âŒ åƒèˆ‡è€…{participant_id_num}é‡è¤‡é€£æ¥ç¬¬{participant_round}è¼ªï¼Œæ‹’çµ•é€£æ¥")
                send_msg(client_socket, {
                    'status': 'rejected', 
                    'reason': 'duplicate_connection', 
                    'expected': expected_round, 
                    'received': participant_round
                })
                client_socket.close()
                return
            
            # è¨˜éŒ„æ–°çš„åƒèˆ‡è€…é€£æ¥
            connected_participant_ids.add(participant_id_num)
            connected_participants_count += 1
            logging.info(f"âœ… è¼ªæ¬¡é©—è­‰é€šéï¼š{participant_id} (åƒèˆ‡è€…{participant_id_num}) ç¬¬{participant_round}è¼ªé€£æ¥")
            logging.info(f"åƒèˆ‡è€… {participant_id} (ID: {participant_id_num}) å·²é€£æ¥ (ç•¶å‰å”¯ä¸€é€£æ¥æ•¸: {connected_participants_count})")
        
        # ç™¼é€ç¢ºèªä¿¡æ¯
        send_msg(client_socket, {'status': 'accepted'})

        # Send global model
        global_model_state = global_model.state_dict()
        success = send_msg(client_socket, global_model_state)
        if not success:
             logging.error(f"Failed sending model to {participant_id}")
             return
        logging.info(f"å…¨å±€æ¨¡å‹å·²ç™¼é€åˆ° {participant_id} (åƒèˆ‡è€…{participant_id_num})")

        # Receive model update
        update = recv_msg(client_socket)

        if update is not None and isinstance(update, dict):
            logging.info(f"æ”¶åˆ°ä¾†è‡ª {participant_id} (åƒèˆ‡è€…{participant_id_num}) çš„æ¨¡å‹æ›´æ–°")
            with updates_lock:
                client_updates.append((update, client_socket, participant_id, participant_id_num))  # ğŸ”¥ ä¿å­˜é€£æ¥ä»¥ä¾¿ç¨å¾Œç™¼é€ç¢ºèª
                logging.info(f"ç•¶å‰å·²æ”¶åˆ° {len(client_updates)} å€‹æ›´æ–°")
        else:
            logging.warning(f"å¾ {participant_id} (åƒèˆ‡è€…{participant_id_num}) æ¥æ”¶åˆ°ç„¡æ•ˆæ›´æ–°")
            client_socket.close()
            return

    except Exception as e:
        logging.error(f"è™•ç† {participant_id} æ™‚å‡ºéŒ¯: {e}")
        try:
            client_socket.close()
        except:
            pass
    finally:
        with processed_participants_lock:
            processed_participants_count += 1
            logging.info(f"å·²è™•ç† {processed_participants_count} å€‹åƒèˆ‡è€…")
            expected = get_expected_participants(expected_round)
            if processed_participants_count >= expected and len(client_updates) >= expected:
                logging.info(f"æ‰€æœ‰ {expected} å€‹åƒèˆ‡è€…éƒ½å·²å®Œæˆè¨“ç·´ä¸¦æäº¤æ›´æ–°")
                round_event.set()
            else:
                logging.info(f"ç­‰å¾…æ›´å¤šåƒèˆ‡è€…å®Œæˆè¨“ç·´... (ç•¶å‰: {len(client_updates)}/{expected})")

def aggregate_updates(global_model, update_data, round_num):
    if not update_data:
        logging.warning("æ²’æœ‰æ”¶åˆ°ä»»ä½•æ›´æ–°")
        return global_model

    # ğŸ”¥ åˆ†é›¢æ›´æ–°æ•¸æ“šå’Œé€£æ¥ä¿¡æ¯
    updates = []
    connections = []
    for item in update_data:
        if len(item) == 4:  # (update, socket, participant_id, participant_id_num)
            update, socket, p_id, p_id_num = item
            updates.append(update)
            connections.append((socket, p_id, p_id_num))
        else:
            # å…¼å®¹èˆŠæ ¼å¼
            updates.append(item)
            connections.append(None)

    aggregated_state_dict = {k: torch.zeros_like(v) for k, v in global_model.state_dict().items()} 
    num_updates = len(updates)

    logging.info(f"èšåˆä¾†è‡ª {num_updates} å€‹åƒèˆ‡è€…çš„æ›´æ–°...")
    num_valid_updates = 0
    
    for update in updates:
        if isinstance(update, dict):
            try:
                if update.keys() == aggregated_state_dict.keys():
                    for key in aggregated_state_dict.keys():
                        aggregated_state_dict[key] += update[key].to(device) 
                    num_valid_updates += 1
                else:
                    logging.warning("æ¨¡å‹éµä¸åŒ¹é…ï¼Œè·³éæ­¤æ›´æ–°")
            except Exception as e:
                logging.error(f"èšåˆæ›´æ–°æ™‚å‡ºéŒ¯: {e}")

    if num_valid_updates > 0:
        # è¨ˆç®—å¹³å‡å€¼
        for key in aggregated_state_dict.keys():
            aggregated_state_dict[key] = aggregated_state_dict[key] / num_valid_updates
        
        # æ›´æ–°å…¨å±€æ¨¡å‹
        global_model.load_state_dict(aggregated_state_dict)
        logging.info(f"æ¨¡å‹èšåˆå®Œæˆï¼Œä½¿ç”¨äº† {num_valid_updates} å€‹æœ‰æ•ˆæ›´æ–°")
    else:
        logging.warning("æ²’æœ‰æœ‰æ•ˆçš„æ›´æ–°ç”¨æ–¼èšåˆ")

    # ğŸ”¥ ç™¼é€è¼ªæ¬¡å®Œæˆç¢ºèªçµ¦æ‰€æœ‰åƒèˆ‡è€…
    completion_msg = {'status': 'round_completed', 'round_num': round_num}
    for connection_info in connections:
        if connection_info is not None:
            socket, p_id, p_id_num = connection_info
            try:
                send_msg(socket, completion_msg)
                logging.info(f"âœ… å·²ç™¼é€ç¬¬{round_num}è¼ªå®Œæˆç¢ºèªçµ¦åƒèˆ‡è€…{p_id_num}")
                socket.close()
            except Exception as e:
                logging.warning(f"âš ï¸  ç™¼é€è¼ªæ¬¡å®Œæˆç¢ºèªçµ¦åƒèˆ‡è€…{p_id_num}å¤±æ•—: {e}")
                try:
                    socket.close()
                except:
                    pass

    del updates, aggregated_state_dict
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return global_model

def save_round_result(round_num, accuracy, loss, timestamp):
    """é€è¼ªè¿½åŠ ä¿å­˜çµæœåˆ°CSVæ–‡ä»¶"""
    try:
        with open(results_file, 'a') as f:
            f.write(f"{round_num},{timestamp:.2f},{accuracy:.2f},{loss:.4f}\n")
        
        logging.info(f"ç¬¬ {round_num} è¼ªçµæœå·²è¿½åŠ ä¿å­˜åˆ° {results_file}")
    except Exception as e:
        logging.error(f"ä¿å­˜ç¬¬ {round_num} è¼ªçµæœæ™‚å‡ºéŒ¯: {e}")

def save_accuracy_history(accuracies, filename):
    """ä¿å­˜æº–ç¢ºç‡æ­·å²"""
    try:
        with open(filename, 'w') as f:
            f.write('Round,Accuracy\n')
            for i, acc in enumerate(accuracies, 1):
                f.write(f'{i},{acc:.2f}\n')
    except Exception as e:
        logging.error(f"ä¿å­˜æº–ç¢ºç‡æ­·å²æ™‚å‡ºéŒ¯: {e}")

def main():
    global client_updates, processed_participants_count, connected_participants_count, global_model
    
    # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
    global_model = CNNMnist().to(device)
    
    # Load test data
    logging.info("Loading MNIST test data...")
    try:
        test_data = torch.load(os.path.join('./data', 'mnist_test.pt'), weights_only=False)
        test_loader = load_data('./data')
        logging.info(f"Test data loaded: {len(test_data['data'])} samples")
    except Exception as e:
        logging.error(f"Error loading test data: {e}")
        return

    # Start training
    logging.info("é–‹å§‹è¨“ç·´...")
    
    accuracies = []
    losses = []
    timestamps = []
    total_start_time = time.time()
    
    # ğŸ”¥ æ–°å¢ï¼šè·³éè¼ªæ¬¡è¨ˆæ•¸å™¨
    skipped_rounds = 0

    NUM_ROUNDS = 20
    for round_num in range(1, NUM_ROUNDS + 1):
        logging.info(f"\n=== ğŸ”¥ é–‹å§‹ç¬¬ {round_num} è¼ªè¨“ç·´ (è¼ªæ¬¡é©—è­‰å·²å•Ÿç”¨) ===")
        
        expected_participants = get_expected_participants(round_num)
        logging.info(f"ç¬¬ {round_num} è¼ªé æœŸåƒèˆ‡è€…æ•¸é‡: {expected_participants} (å§‹çµ‚è¦æ±‚å…¨éƒ¨åƒèˆ‡è€…)")
        
        # Reset for this round
        client_updates = []
        processed_participants_count = 0
        connected_participants_count = 0
        connected_participant_ids.clear()  # ğŸ”¥ æ¸…ç†å·²é€£æ¥åƒèˆ‡è€…IDé›†åˆ
        round_event.clear()

        # Start server socket
        server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        server_socket.bind(('localhost', 9998))
        server_socket.listen(expected_participants)
        server_socket.settimeout(1.0)
        logging.info("Server listening on port 9998 with round verification")

        # Accept connections with fault detection
        threads = []
        connected_count = 0
        FAULT_DETECTION_TIMEOUT = 30  # 30ç§’æ•…éšœåµæ¸¬
        connection_start_time = time.time()
        
        logging.info(f"ç­‰å¾…æ‰€æœ‰ {expected_participants} å€‹åƒèˆ‡è€…é€£æ¥ç¬¬{round_num}è¼ªï¼ˆ{FAULT_DETECTION_TIMEOUT}ç§’è¶…æ™‚ï¼‰...")
        
        # ç­‰å¾…å¾ªç’°
        while connected_count < expected_participants:
            elapsed_time = time.time() - connection_start_time
            
            if elapsed_time > FAULT_DETECTION_TIMEOUT:
                logging.warning(f"âš ï¸  ç¬¬ {round_num} è¼ªæ•…éšœåµæ¸¬ï¼š{FAULT_DETECTION_TIMEOUT}ç§’å…§åªæ”¶åˆ° {connected_count}/{expected_participants} å€‹åƒèˆ‡è€…")
                if round_num == 8:
                    logging.info(f"ğŸ”¥ ç¬¬8è¼ªæ•…éšœå®¹éŒ¯ï¼šåµæ¸¬åˆ°åƒèˆ‡è€…1å’Œ2æ•…éšœï¼Œä½¿ç”¨ {connected_count} å€‹å¯ç”¨åƒèˆ‡è€…ç¹¼çºŒè¨“ç·´")
                    break
                elif connected_count >= 1:
                    logging.info(f"âœ… ç¬¬ {round_num} è¼ªç¹¼çºŒé€²è¡Œï¼šæœ‰ {connected_count} å€‹åƒèˆ‡è€…åƒèˆ‡")
                    break
                else:
                    logging.error(f"âŒ ç¬¬ {round_num} è¼ªç„¡åƒèˆ‡è€…ï¼Œçµ‚æ­¢å¯¦é©—")
                    return
            
            try:
                client_socket, client_address = server_socket.accept()
                logging.info(f"æ¥å—ä¾†è‡ª {client_address} çš„é€£æ¥ï¼Œç­‰å¾…è¼ªæ¬¡é©—è­‰...")
                
                # å‰µå»ºç·šç¨‹è™•ç†å®¢æˆ¶ç«¯ï¼ŒåŒ…å«è¼ªæ¬¡é©—è­‰
                thread = threading.Thread(target=handle_client, args=(client_socket, client_address, round_num))
                thread.start()
                threads.append(thread)
                
                # ç­‰å¾…ä¸€ä¸‹è®“è¼ªæ¬¡é©—è­‰å®Œæˆ
                time.sleep(0.5)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰æ–°çš„æœ‰æ•ˆå”¯ä¸€é€£æ¥
                with connection_lock:
                    unique_connections = len(connected_participant_ids)
                if unique_connections > connected_count:
                    connected_count = unique_connections
                    logging.info(f"âœ… ç¬¬{round_num}è¼ªå·²é©—è­‰å”¯ä¸€é€£æ¥æ•¸: {connected_count}/{expected_participants}")
                
            except socket.timeout:
                continue
            except Exception as e:
                logging.error(f"æ¥å—é€£æ¥æ™‚å‡ºéŒ¯: {e}")
                continue

        # ç¬¬8è¼ªç‰¹æ®Šè™•ç†ï¼šå¼·åˆ¶ç­‰å¾…å®Œæ•´30ç§’é€²è¡Œæ•…éšœåµæ¸¬
        if round_num == 8:
            remaining_time = FAULT_DETECTION_TIMEOUT - (time.time() - connection_start_time)
            if remaining_time > 0:
                logging.info(f"ğŸ”¥ ç¬¬8è¼ªæ•…éšœå®¹éŒ¯ï¼šå¼·åˆ¶ç­‰å¾…å‰©é¤˜ {remaining_time:.1f} ç§’å®Œæˆæ•…éšœåµæ¸¬")
                time.sleep(remaining_time)
            
            # æ•…éšœæ¢å¾©ï¼šå¾ç¬¬7è¼ªcheckpointæ¢å¾©æ¨¡å‹
            logging.info(f"ğŸ”¥ ç¬¬8è¼ªæ•…éšœæ¢å¾©ï¼šåµæ¸¬åˆ°åƒèˆ‡è€…1å’Œ2æ•…éšœï¼Œå¾ç¬¬7è¼ªcheckpointæ¢å¾©æ¨¡å‹")
            try:
                checkpoint_path = os.path.join(RESULTS_DIR, f"model_round_7.pth")
                if os.path.exists(checkpoint_path):
                    global_model.load_state_dict(torch.load(checkpoint_path, map_location=device, weights_only=False))
                    logging.info(f"âœ… å·²å¾ {checkpoint_path} æ¢å¾©æ¨¡å‹ç‹€æ…‹")
                else:
                    logging.warning(f"âš ï¸  æœªæ‰¾åˆ°checkpoint {checkpoint_path}ï¼Œä½¿ç”¨ç•¶å‰æ¨¡å‹ç‹€æ…‹")
            except Exception as e:
                logging.error(f"âŒ æ•…éšœæ¢å¾©å¤±æ•—: {e}")
            
            logging.info(f"ğŸ”¥ ç¬¬8è¼ªæ•…éšœæ¢å¾©å®Œæˆï¼šä½¿ç”¨ {connected_count} å€‹å¯ç”¨åƒèˆ‡è€…ç¹¼çºŒè¨“ç·´")

        # æ‰€æœ‰åƒèˆ‡è€…å·²é€£æ¥ï¼ˆæˆ–é”åˆ°è¶…æ™‚ï¼‰
        training_start_time = time.time()
        logging.info(f"âœ… ç¬¬{round_num}è¼ªé–‹å§‹è¨“ç·´ï¼š{connected_count}/{expected_participants} å€‹åƒèˆ‡è€…åƒèˆ‡")

        # Wait for all participants to finish
        logging.info(f"ç­‰å¾… {connected_count} å€‹åƒèˆ‡è€…å®Œæˆç¬¬{round_num}è¼ªè¨“ç·´...")
        timeout_seconds = 30
        
        while True:
            with updates_lock:
                current_updates = len(client_updates)
            
            logging.info(f"ç•¶å‰å·²æ”¶åˆ° {current_updates}/{connected_count} å€‹ç¬¬{round_num}è¼ªåƒèˆ‡è€…çš„æ›´æ–°")
            
            if current_updates >= connected_count:
                logging.info(f"âœ“ æ‰€æœ‰ {connected_count} å€‹ç¬¬{round_num}è¼ªåƒèˆ‡è€…éƒ½å·²å®Œæˆè¨“ç·´ä¸¦æäº¤æ›´æ–°")
                break
            
            if time.time() - training_start_time > timeout_seconds:
                logging.error(f"âœ— è¶…æ™‚ç­‰å¾…ç¬¬{round_num}è¼ªåƒèˆ‡è€…å®Œæˆ (è¶…é {timeout_seconds} ç§’), åªæ”¶åˆ° {current_updates}/{connected_count} å€‹æ›´æ–°")
                logging.error("ç‚ºç¢ºä¿å¯¦é©—æº–ç¢ºæ€§ï¼Œæœå‹™å™¨å°‡çµ‚æ­¢æœ¬è¼ªè¨“ç·´")
                server_socket.close()
                for thread in threads:
                    thread.join(timeout=5)
                logging.error("æœå‹™å™¨å› ç‚ºè¨“ç·´è¶…æ™‚è€Œé€€å‡º")
                return
            
            time.sleep(1)

        training_end_time = time.time()
        
        # Close server socket
        server_socket.close()

        # Wait for all threads to finish
        for thread in threads:
            thread.join()

        # Aggregate updates - ä½¿ç”¨å¯¦éš›æ”¶åˆ°çš„æ›´æ–°æ•¸é‡
        if len(client_updates) >= 1:  # è‡³å°‘æœ‰1å€‹æ›´æ–°å°±é€²è¡Œèšåˆ
            logging.info(f"âœ“ æ­£åœ¨èšåˆä¾†è‡ª {len(client_updates)} å€‹ç¬¬{round_num}è¼ªåƒèˆ‡è€…çš„æ›´æ–°")
            global_model = aggregate_updates(global_model, client_updates, round_num)
            logging.info("âœ“ æ¨¡å‹èšåˆå®Œæˆï¼Œå…¨å±€æ¨¡å‹å·²æ›´æ–°")
        else:
            logging.error(f"âœ— æ²’æœ‰æ”¶åˆ°ä»»ä½•ç¬¬{round_num}è¼ªæ›´æ–°ï¼Œè·³éæœ¬è¼ª")
            continue

        # Evaluate model
        accuracy, loss = evaluate_model(global_model, test_loader, device)
        
        round_duration = training_end_time - training_start_time
        cumulative_time = time.time() - total_start_time
        
        # ğŸ”¥ ä½¿ç”¨å¯¦éš›è¼ªæ¬¡ç·¨è™Ÿä¿å­˜çµæœï¼ˆä¸è¦èª¿æ•´ï¼‰
        logging.info(f"ğŸ’¾ ä¿å­˜çµæœï¼šç¬¬{round_num}è¼ªè¨“ç·´å®Œæˆ")
        save_round_result(round_num, accuracy, loss, cumulative_time)
        
        accuracies.append(accuracy)
        losses.append(loss)
        timestamps.append(cumulative_time)
        save_accuracy_history(accuracies, accuracy_history_file)
        logging.info(f"ç¬¬ {round_num} è¼ªæº–ç¢ºç‡ï¼š{accuracy:.2f}%")
        
        # Save checkpoint (ä½¿ç”¨å¯¦éš›è¼ªæ¬¡ç·¨è™Ÿ)
        checkpoint_path = os.path.join(RESULTS_DIR, f"model_round_{round_num}.pth")
        torch.save(global_model.state_dict(), checkpoint_path)
        logging.info(f"ç¬¬ {round_num} è¼ªcheckpointå·²ä¿å­˜: {checkpoint_path}")
        
        logging.info(f"Round {round_num} completed in {round_duration:.2f} seconds (actual training time).")
        logging.info(f"  - Expected participants: {expected_participants}")
        logging.info(f"  - Actual participants: {len(client_updates)}")
        logging.info(f"  - Accuracy: {accuracy:.2f}%")
        logging.info(f"  - Checkpoint saved: {checkpoint_path}")

    logging.info("\nğŸ‰ æ‰€æœ‰20è¼ªè¯é‚¦å­¸ç¿’å®Œæˆï¼")
    logging.info(f"ğŸ“Š æœ€çµ‚æº–ç¢ºç‡: {accuracies[-1]:.2f}%")
    logging.info(f"ğŸ“ çµæœå·²ä¿å­˜åˆ°: {results_file}")
    logging.info(f"ğŸ“ˆ å®Œæˆ {NUM_ROUNDS} è¼ªè¨“ç·´")

if __name__ == '__main__':
    main() 