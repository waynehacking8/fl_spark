#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Spark FL Implementation for CIFAR-10 with RDD Lineage Fault Tolerance (Improved)
åŸºæ–¼Sparkçš„CIFAR-10è¯é‚¦å­¸ç¿’å¯¦ç¾ï¼Œæ”¯æŒRDDè¡€çµ±è¿½è¹¤å®¹éŒ¯ï¼ˆæ”¹é€²ç‰ˆï¼‰
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import time
import logging
import os
import sys
from pyspark.sql import SparkSession
from pyspark.broadcast import Broadcast
import pickle

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘ä»¥å°å…¥çµ±ä¸€æ¨¡å‹
sys.path.append('..')
from models import CIFAR10ResNet, CIFAR10CNNSimple

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - CIFAR10-SparkFL-Improved - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)

def precise_federated_averaging(model_updates, weights, reference_model):
    """
    ç²¾ç¢ºçš„è¯é‚¦å¹³å‡ç®—æ³•ï¼Œèˆ‡å‚³çµ±FLä¿æŒä¸€è‡´
    """
    if not model_updates:
        logging.warning("æ²’æœ‰æ¨¡å‹æ›´æ–°ï¼Œè·³éèšåˆ")
        return None
    
    # è¨ˆç®—æ­¸ä¸€åŒ–æ¬Šé‡
    total_samples = sum(weights)
    normalized_weights = [w / total_samples for w in weights]
    
    # ç²å–åƒè€ƒæ¨¡å‹çš„ç‹€æ…‹å­—å…¸ä½œç‚ºæ¨¡æ¿
    reference_state = reference_model.state_dict()
    global_state_dict = {}
    
    # ç²å–è¨­å‚™ä¿¡æ¯
    device = next(reference_model.parameters()).device
    
    # åˆå§‹åŒ– - ä¿æŒæ•¸æ“šé¡å‹
    for key in model_updates[0].keys():
        global_state_dict[key] = torch.zeros_like(reference_state[key]).to(device)
    
    # ç²¾ç¢ºåŠ æ¬Šå¹³å‡ - å…ˆè½‰ç‚ºfloatè¨ˆç®—å†è½‰å›åŸé¡å‹
    for i, update in enumerate(model_updates):
        weight = normalized_weights[i]
        for key in update.keys():
            # ç²å–åŸå§‹æ•¸æ“šé¡å‹
            original_dtype = global_state_dict[key].dtype
            param_tensor = update[key].to(device)  # ç¢ºä¿åœ¨æ­£ç¢ºè¨­å‚™ä¸Š
            
            # è½‰ç‚ºfloaté€²è¡Œç²¾ç¢ºè¨ˆç®—
            if param_tensor.dtype != torch.float32:
                param_tensor = param_tensor.float()
            if global_state_dict[key].dtype != torch.float32:
                global_state_dict[key] = global_state_dict[key].float()
            
            # åŸ·è¡ŒåŠ æ¬Šæ±‚å’Œ
            global_state_dict[key] += weight * param_tensor
            
            # è½‰å›åŸå§‹é¡å‹
            if original_dtype != torch.float32:
                global_state_dict[key] = global_state_dict[key].to(original_dtype)
    
    logging.info(f"ç²¾ç¢ºè¯é‚¦å¹³å‡å®Œæˆï¼Œä½¿ç”¨ {len(model_updates)} å€‹æ›´æ–°")
    return global_state_dict

def train_federated_learning_spark(num_rounds=20, num_partitions=2, model_type='resnet', 
                                  batch_size=32, learning_rate=0.001, local_epochs=5,
                                  experiment_mode='normal'):
    """
    ä½¿ç”¨Apache Sparké€²è¡Œè¯é‚¦å­¸ç¿’è¨“ç·´ (CIFAR-10) - æ”¹é€²ç‰ˆ
    """
    # å¯¦é©—æ¨¡å¼é…ç½®
    if experiment_mode == 'normal':
        fault_round = None
        failed_partition = None
        print("ğŸ”„ æ­£å¸¸æ¨¡å¼ï¼šç„¡æ•…éšœæ³¨å…¥")
    elif experiment_mode == 'exp1':
        fault_round = 5
        failed_partition = 0
        print("ğŸ§ª å¯¦é©—1æ¨¡å¼ï¼šç¬¬5è¼ªæ•¸æ“šåˆ†ç‰‡è²¢ç»å¤±æ•— (partition 0)")
    elif experiment_mode == 'exp2':
        fault_round = 8
        failed_partition = 0
        print("ğŸ”§ å¯¦é©—2æ¨¡å¼ï¼šç¬¬8è¼ªWorkerç¯€é»æ•…éšœ (partition 0)")
    else:
        raise ValueError(f"Unknown experiment mode: {experiment_mode}")
    
    # è¨­å‚™é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    # æª¢æŸ¥æ•¸æ“šæ–‡ä»¶
    train_data_files = [
        "../data/cifar10_train_part1.pt",
        "../data/cifar10_train_part2.pt"
    ]
    test_data_file = "../data/cifar10_test.pt"
    
    for file_path in train_data_files + [test_data_file]:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    print("âœ… æ•¸æ“šæ–‡ä»¶æª¢æŸ¥å®Œæˆ")
    
    # åˆå§‹åŒ–Spark
    spark = SparkSession.builder \
        .appName(f"CIFAR10FederatedLearning_Improved_{experiment_mode}") \
        .master("local[2]") \
        .config("spark.driver.memory", "12g") \
        .config("spark.executor.memory", "12g") \
        .config("spark.driver.maxResultSize", "4g") \
        .config("spark.python.worker.memory", "8g") \
        .config("spark.driver.memoryFraction", "0.8") \
        .config("spark.executor.memoryFraction", "0.8") \
        .config("spark.storage.memoryFraction", "0.6") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.kryo.unsafe", "true") \
        .config("spark.kryo.maxCacheSize", "1g") \
        .config("spark.kryoserializer.buffer.max", "1g") \
        .config("spark.kryoserializer.buffer", "64m") \
        .config("spark.rdd.compress", "true") \
        .config("spark.broadcast.compress", "true") \
        .config("spark.io.compression.codec", "lz4") \
        .config("spark.default.parallelism", "2") \
        .config("spark.sql.adaptive.enabled", "false") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "false") \
        .config("spark.python.worker.reuse", "false") \
        .config("spark.executor.cores", "2") \
        .config("spark.cores.max", "2") \
        .config("spark.executor.memoryStorageLevel", "MEMORY_ONLY") \
        .config("spark.serializer.objectStreamReset", "100") \
        .getOrCreate()
    
    sc = spark.sparkContext
    sc.setLogLevel("WARN")
    
    print(f"ğŸš€ Spark åˆå§‹åŒ–å®Œæˆ ({experiment_mode}æ¨¡å¼) - æ”¹é€²ç‰ˆ")
    
    # å‰µå»ºçµæœç›®éŒ„
    results_dir = f"../results/spark/{experiment_mode}"
    os.makedirs(results_dir, exist_ok=True)
    
    # åˆå§‹åŒ–çµæœæ–‡ä»¶ - ç›´æ¥è¦†è“‹åŸæ–‡ä»¶
    results_file = os.path.join(results_dir, f'cifar10_spark_{experiment_mode}_results.csv')
    with open(results_file, 'w') as f:
        f.write("Round,Timestamp,Accuracy,Loss,Active_Partitions,Failed_Partitions,Mode\n")
    
    # åˆå§‹åŒ–å…¨å±€æ¨¡å‹ - ä½¿ç”¨çµ±ä¸€æ¨¡å‹å®šç¾©
    if model_type == 'simple':
        global_model = CIFAR10CNNSimple(num_classes=10).to(device)
    else:
        global_model = CIFAR10ResNet(num_classes=10, dropout_rate=0.3).to(device)
    
    criterion = nn.CrossEntropyLoss()
    
    # åŠ è¼‰æ¸¬è©¦æ•¸æ“š
    test_data = torch.load(test_data_file, weights_only=False)
    test_dataset = TensorDataset(test_data['data'], test_data['targets'])
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    # å‰µå»ºåˆ†å¸ƒå¼æ•¸æ“šRDDï¼ˆåˆ†æ‰¹åŠ è¼‰ä»¥æ¸›å°‘å…§å­˜å£“åŠ›ï¼‰
    partition_info = []
    for i, file_path in enumerate(train_data_files):
        if i < num_partitions:
            # åªä¿å­˜æ–‡ä»¶è·¯å¾‘ï¼Œä¸ç›´æ¥åŠ è¼‰å¤§æ•¸æ“š
            partition_info.append((i, file_path))
    
    # å‰µå»ºè¼ƒå°çš„RDDï¼Œåƒ…åŒ…å«åˆ†å€ä¿¡æ¯
    data_rdd = sc.parallelize(partition_info, num_partitions)
    data_rdd.cache()  # å•Ÿç”¨ç·©å­˜
    
    print(f"ğŸ“Š å‰µå»º {num_partitions} å€‹åˆ†å€çš„åˆ†å¸ƒå¼æ•¸æ“šé›†ï¼ˆæ‡¶åŠ è¼‰æ¨¡å¼ï¼‰")
    
    # è¨˜éŒ„ç¸½é–‹å§‹æ™‚é–“
    total_start_time = time.time()
    
    # è¯é‚¦å­¸ç¿’ä¸»å¾ªç’°
    for round_num in range(1, num_rounds + 1):
        print(f"\n=== ç¬¬ {round_num} è¼ªé–‹å§‹ ({experiment_mode}æ¨¡å¼) - æ”¹é€²ç‰ˆ ===")
        round_start_time = time.time()
        
        # å»£æ’­å…¨å±€æ¨¡å‹åƒæ•¸
        model_state = global_model.state_dict()
        broadcasted_model = sc.broadcast(model_state)
        
        # æ•…éšœæ³¨å…¥
        if fault_round is not None and round_num == fault_round and failed_partition is not None:
            if experiment_mode == 'exp1':
                print(f"ğŸ§ª ç¬¬ {round_num} è¼ªæ•¸æ“šåˆ†ç‰‡è²¢ç»å¤±æ•—ï¼špartition {failed_partition} æ•…éšœ")
            elif experiment_mode == 'exp2':
                print(f"ğŸ”§ ç¬¬ {round_num} è¼ªWorkerç¯€é»æ•…éšœï¼špartition {failed_partition} æ•…éšœ")
            
            # éæ¿¾æ•…éšœåˆ†å€
            active_rdd = data_rdd.filter(lambda x: x[0] != failed_partition)
        else:
            active_rdd = data_rdd
        
        # åˆ†å¸ƒå¼è¨“ç·´å‡½æ•¸ - æ”¹é€²ç‰ˆ
        def train_partition_improved(partition_info):
            import torch
            import torch.nn as nn
            import torch.optim as optim
            from torch.utils.data import DataLoader, TensorDataset
            import numpy as np
            import sys
            import os
            
            # æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘
            current_dir = os.path.dirname(os.path.abspath(__file__))
            parent_dir = os.path.dirname(current_dir)
            if parent_dir not in sys.path:
                sys.path.append(parent_dir)
            
            # å°å…¥çµ±ä¸€æ¨¡å‹å®šç¾©
            try:
                from models import CIFAR10ResNet, CIFAR10CNNSimple
            except ImportError:
                # å¦‚æœå°å…¥å¤±æ•—ï¼Œä½¿ç”¨å…§è¯å®šç¾©ä½œç‚ºå‚™ä»½
                class ResidualBlock(nn.Module):
                    def __init__(self, in_channels, out_channels, stride=1, dropout_rate=0.1):
                        super(ResidualBlock, self).__init__()
                        
                        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)
                        self.bn1 = nn.BatchNorm2d(out_channels)
                        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False)
                        self.bn2 = nn.BatchNorm2d(out_channels)
                        
                        self.shortcut = nn.Sequential()
                        if stride != 1 or in_channels != out_channels:
                            self.shortcut = nn.Sequential(
                                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),
                                nn.BatchNorm2d(out_channels)
                            )
                        
                        self.dropout = nn.Dropout2d(dropout_rate)
                    
                    def forward(self, x):
                        out = torch.relu(self.bn1(self.conv1(x)))
                        out = self.bn2(self.conv2(out))
                        out += self.shortcut(x)
                        out = torch.relu(out)
                        out = self.dropout(out)
                        return out

                class CIFAR10ResNet(nn.Module):
                    def __init__(self, num_classes=10, dropout_rate=0.3):
                        super(CIFAR10ResNet, self).__init__()
                        
                        self.conv1 = nn.Conv2d(3, 32, 3, stride=1, padding=1, bias=False)
                        self.bn1 = nn.BatchNorm2d(32)
                        self.dropout1 = nn.Dropout2d(dropout_rate * 0.5)
                        
                        self.layer1 = self._make_layer(32, 32, 2, stride=1, dropout_rate=dropout_rate * 0.5)
                        self.layer2 = self._make_layer(32, 64, 2, stride=2, dropout_rate=dropout_rate * 0.7)
                        self.layer3 = self._make_layer(64, 128, 2, stride=2, dropout_rate=dropout_rate)
                        
                        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
                        self.dropout_fc = nn.Dropout(dropout_rate)
                        self.fc = nn.Linear(128, num_classes)
                        
                    def _make_layer(self, in_channels, out_channels, num_blocks, stride, dropout_rate=0.1):
                        layers = []
                        layers.append(ResidualBlock(in_channels, out_channels, stride, dropout_rate))
                        for _ in range(1, num_blocks):
                            layers.append(ResidualBlock(out_channels, out_channels, dropout_rate=dropout_rate))
                        return nn.Sequential(*layers)
                    
                    def forward(self, x):
                        x = torch.relu(self.bn1(self.conv1(x)))
                        x = self.dropout1(x)
                        x = self.layer1(x)
                        x = self.layer2(x)
                        x = self.layer3(x)
                        x = self.avg_pool(x)
                        x = x.view(x.size(0), -1)
                        x = self.dropout_fc(x)
                        x = self.fc(x)
                        return x
            
            partition_id, file_path = partition_info
            
            try:
                # æª¢æŸ¥åˆ†å€æ˜¯å¦åœ¨æ•…éšœåˆ—è¡¨ä¸­
                if (fault_round is not None and round_num == fault_round and 
                    failed_partition is not None and partition_id == failed_partition):
                    if experiment_mode == 'exp1':
                        print(f"ğŸ’¥ Partition {partition_id} æ¨¡æ“¬æ•¸æ“šåˆ†ç‰‡è²¢ç»å¤±æ•—")
                    elif experiment_mode == 'exp2':
                        print(f"ğŸ’¥ Partition {partition_id} æ¨¡æ“¬Workerç¯€é»æ•…éšœ")
                    raise RuntimeError(f"Simulated partition {partition_id} failure")
                
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                
                # å‰µå»ºæœ¬åœ°æ¨¡å‹ - ä½¿ç”¨çµ±ä¸€å®šç¾©
                if model_type == 'simple':
                    local_model = CIFAR10CNNSimple(num_classes=10).to(device)
                else:
                    local_model = CIFAR10ResNet(num_classes=10, dropout_rate=0.3).to(device)
                
                local_model.load_state_dict(broadcasted_model.value)
                local_model.train()
                
                # æº–å‚™æ•¸æ“š
                data = torch.load(file_path, weights_only=False)
                dataset = TensorDataset(data['data'], data['targets'])
                data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
                
                # æ”¹é€²çš„å„ªåŒ–å™¨é…ç½® - æ·»åŠ L2æ­£å‰‡åŒ–
                weight_decay = 1e-4  # L2æ­£å‰‡åŒ–
                optimizer = optim.Adam(local_model.parameters(), lr=learning_rate, weight_decay=weight_decay)
                
                # å­¸ç¿’ç‡èª¿åº¦å™¨ - æ¯5è¼ªæ¸›å°‘å­¸ç¿’ç‡
                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)
                
                criterion = nn.CrossEntropyLoss()
                
                # æœ¬åœ°è¨“ç·´
                local_losses = []
                for epoch in range(local_epochs):
                    epoch_losses = []
                    for batch_data, batch_targets in data_loader:
                        batch_data, batch_targets = batch_data.to(device), batch_targets.to(device)
                        
                        optimizer.zero_grad()
                        outputs = local_model(batch_data)
                        loss = criterion(outputs, batch_targets)
                        loss.backward()
                        optimizer.step()
                        
                        epoch_losses.append(loss.item())
                    
                    local_losses.append(np.mean(epoch_losses))
                
                # æ›´æ–°å­¸ç¿’ç‡èª¿åº¦å™¨
                scheduler.step()
                current_lr = optimizer.param_groups[0]['lr']
                
                # è¿”å›æ¨¡å‹åƒæ•¸å’Œè¨“ç·´ä¿¡æ¯ - ç¢ºä¿æ•¸æ“šé¡å‹ä¸€è‡´æ€§
                model_state_cpu = {}
                for key, value in local_model.state_dict().items():
                    model_state_cpu[key] = value.cpu().clone()
                
                return {
                    'partition_id': partition_id,
                    'model_state': model_state_cpu,
                    'data_size': len(dataset),
                    'avg_loss': np.mean(local_losses),
                    'current_lr': current_lr,
                    'status': 'success'
                }
                
            except Exception as e:
                # æ¨¡æ“¬æ•…éšœæ¢å¾©
                print(f"âš ï¸  Partition {partition_id} è¨“ç·´å¤±æ•—: {str(e)}")
                return {
                    'partition_id': partition_id,
                    'status': 'failed',
                    'error': str(e)
                }
        
        # åŸ·è¡Œåˆ†å¸ƒå¼è¨“ç·´ä¸¦æ”¶é›†çµæœ
        try:
            training_results = active_rdd.map(train_partition_improved).collect()
        except Exception as e:
            print(f"âŒ åˆ†å¸ƒå¼è¨“ç·´å¤±æ•—: {e}")
            # åˆ©ç”¨RDD lineageé€²è¡Œæ•…éšœæ¢å¾©
            print("ğŸ”„ ä½¿ç”¨RDD lineageé€²è¡Œæ•…éšœæ¢å¾©...")
            training_results = active_rdd.map(train_partition_improved).collect()
        
        # éæ¿¾æˆåŠŸçš„çµæœ
        successful_results = [r for r in training_results if r['status'] == 'success']
        failed_results = [r for r in training_results if r['status'] == 'failed']
        
        if failed_results:
            print(f"âš ï¸  {len(failed_results)} å€‹åˆ†å€è¨“ç·´å¤±æ•—ï¼Œä½¿ç”¨ {len(successful_results)} å€‹æˆåŠŸåˆ†å€é€²è¡Œèšåˆ")
        
        if not successful_results:
            print(f"âŒ ç¬¬ {round_num} è¼ªæ²’æœ‰æˆåŠŸçš„åˆ†å€ï¼Œè·³éèšåˆ")
            continue
        
        # ä½¿ç”¨ç²¾ç¢ºè¯é‚¦å¹³å‡ç®—æ³•
        model_updates = [r['model_state'] for r in successful_results]
        weights = [r['data_size'] for r in successful_results]
        
        new_state_dict = precise_federated_averaging(model_updates, weights, global_model)
        
        if new_state_dict is not None:
            # æ›´æ–°å…¨å±€æ¨¡å‹
            global_model.load_state_dict(new_state_dict)
        else:
            print(f"âŒ ç¬¬ {round_num} è¼ªè¯é‚¦å¹³å‡å¤±æ•—")
            continue
        
        # è©•ä¼°æ¨¡å‹
        global_model.eval()
        correct = 0
        total = 0
        total_loss = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                outputs = global_model(data)
                loss = criterion(outputs, target)
                total_loss += loss.item()
                
                _, predicted = torch.max(outputs.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        accuracy = 100 * correct / total
        avg_loss = total_loss / len(test_loader)
        cumulative_time = time.time() - total_start_time  # ä½¿ç”¨ç´¯è¨ˆæ™‚é–“
        
        # è¨˜éŒ„çµæœ
        active_partitions = len(successful_results)
        failed_partitions = len(failed_results)
        
        with open(results_file, 'a') as f:
            f.write(f"{round_num},{cumulative_time:.2f},{accuracy:.2f},{avg_loss:.4f},"
                   f"{active_partitions},{failed_partitions},{experiment_mode}\n")
        
        # ä¿å­˜æ¨¡å‹æª¢æŸ¥é»
        checkpoint_path = os.path.join(results_dir, f'model_round_{round_num}.pth')
        torch.save(global_model.state_dict(), checkpoint_path)
        
        print(f"ç¬¬ {round_num} è¼ªå®Œæˆ:")
        print(f"  æº–ç¢ºç‡: {accuracy:.2f}%")
        print(f"  æå¤±: {avg_loss:.4f}")
        print(f"  ç´¯è¨ˆç”¨æ™‚: {cumulative_time:.2f}ç§’")
        print(f"  æ´»èºåˆ†å€: {active_partitions}/{num_partitions}")
        if failed_partitions > 0:
            print(f"  æ•…éšœåˆ†å€: {failed_partitions}")
        if successful_results:
            avg_lr = np.mean([r.get('current_lr', learning_rate) for r in successful_results])
            print(f"  ç•¶å‰å­¸ç¿’ç‡: {avg_lr:.6f}")
        
        # æ¸…ç†å»£æ’­è®Šé‡
        broadcasted_model.unpersist()
    
    # æ¸…ç†è³‡æº
    data_rdd.unpersist()
    spark.stop()
    
    print("\nğŸ‰ CIFAR-10 Sparkè¯é‚¦å­¸ç¿’å®Œæˆï¼(æ”¹é€²ç‰ˆ)")
    print(f"çµæœä¿å­˜åœ¨: {results_file}")

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description='CIFAR-10 Federated Learning with Apache Spark')
    parser.add_argument('--rounds', type=int, default=20, help='Number of federated rounds')
    parser.add_argument('--partitions', type=int, default=2, help='Number of data partitions')
    parser.add_argument('--model', type=str, default='resnet', choices=['simple', 'resnet'],
                       help='Model type')
    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')
    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')
    parser.add_argument('--epochs', type=int, default=5, help='Local epochs')
    parser.add_argument('--mode', type=str, default='normal', choices=['normal', 'exp1', 'exp2'],
                       help='Experiment mode: normal, exp1 (data shard failure), exp2 (worker failure)')
    
    args = parser.parse_args()
    
    train_federated_learning_spark(
        num_rounds=args.rounds,
        num_partitions=args.partitions,
        model_type=args.model,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        local_epochs=args.epochs,
        experiment_mode=args.mode
    )

if __name__ == "__main__":
    main() 