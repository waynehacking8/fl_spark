# CIFAR-10è¯é‚¦å­¸ç¿’å®¹éŒ¯å¯¦é©—æŠ€è¡“æ–‡æª”

## ğŸ—ï¸ ä»£ç¢¼æ¶æ§‹è¨­è¨ˆ

### æ ¸å¿ƒè¨­è¨ˆåŸå‰‡
1. **æ¨¡çµ„åŒ–è¨­è¨ˆ**: å„çµ„ä»¶ç¨ç«‹ï¼Œä¾¿æ–¼ç¶­è­·å’Œæ“´å±•
2. **çµ±ä¸€æ¥å£**: å‚³çµ±FLå’ŒSpark FLä½¿ç”¨ç›¸åŒçš„æ¨¡å‹å’Œæ•¸æ“šæ ¼å¼
3. **å®¹éŒ¯å„ªå…ˆ**: å…§å»ºå¤šç¨®æ•…éšœæª¢æ¸¬å’Œæ¢å¾©æ©Ÿåˆ¶
4. **å¯é…ç½®æ€§**: æ”¯æ´å¤šç¨®å¯¦é©—æ¨¡å¼å’Œåƒæ•¸èª¿æ•´

## ğŸ“ æ–‡ä»¶çµæ§‹è©³è§£

```
exp3_cifar10/
â”œâ”€â”€ ğŸ“Š æ•¸æ“šå±¤
â”‚   â”œâ”€â”€ prepare_cifar10.py          # æ•¸æ“šä¸‹è¼‰å’Œé è™•ç†
â”‚   â””â”€â”€ data/                       # CIFAR-10æ•¸æ“šé›†å­˜å„²
â”œâ”€â”€ ğŸ§  æ¨¡å‹å±¤  
â”‚   â””â”€â”€ models.py                   # çµ±ä¸€CNN/ResNetæ¨¡å‹å®šç¾©
â”œâ”€â”€ ğŸ”§ å‚³çµ±FLå¯¦ç¾
â”‚   â”œâ”€â”€ server_cifar10.py           # è¯é‚¦æœå‹™å™¨
â”‚   â””â”€â”€ participant_cifar10.py      # è¯é‚¦åƒèˆ‡è€…
â”œâ”€â”€ âš¡ Spark FLå¯¦ç¾
â”‚   â””â”€â”€ spark_fl_cifar10.py         # åŸºæ–¼RDDçš„è¯é‚¦å­¸ç¿’
â”œâ”€â”€ ğŸ§ª å¯¦é©—è…³æœ¬
â”‚   â”œâ”€â”€ run_exp1_experiment.sh      # exp1æ•…éšœå¯¦é©—
â”‚   â”œâ”€â”€ run_20rounds_comparison.sh  # 20è¼ªå°æ¯”å¯¦é©—
â”‚   â””â”€â”€ test_anti_overfitting.sh    # é˜²éæ“¬åˆæ¸¬è©¦
â””â”€â”€ ğŸ“ˆ çµæœåˆ†æ
    â””â”€â”€ results/                    # å¯¦é©—çµæœCSVæ–‡ä»¶
```

## ğŸ” æ ¸å¿ƒä»£ç¢¼åˆ†æ

### 1. æ•¸æ“šæº–å‚™æ¨¡çµ„ (`prepare_cifar10.py`)

#### é—œéµåŠŸèƒ½
```python
def download_and_prepare_cifar10():
    """
    è‡ªå‹•ä¸‹è¼‰CIFAR-10ä¸¦å‰µå»ºè¯é‚¦åˆ†ç‰‡
    - ä¸‹è¼‰åŸå§‹æ•¸æ“šé›†
    - åˆ†å‰²ç‚º2å€‹IIDåˆ†ç‰‡
    - å‰µå»ºæ¸¬è©¦é›†
    - æ•¸æ“šå®Œæ•´æ€§é©—è­‰
    """
    
def create_federated_splits(train_data, train_targets, num_participants=2):
    """
    å‰µå»ºè¯é‚¦åˆ†ç‰‡ï¼Œç¢ºä¿IIDåˆ†å¸ƒ
    - æ¯å€‹é¡åˆ¥å‡å‹»åˆ†é…
    - ä¿æŒæ•¸æ“šå¹³è¡¡
    - ç”Ÿæˆ.ptæ ¼å¼æ–‡ä»¶
    """
```

#### æŠ€è¡“ç‰¹é»
- **è‡ªå‹•åŒ–ä¸‹è¼‰**: ä½¿ç”¨torchvision.datasetsè‡ªå‹•ç²å–CIFAR-10
- **IIDåˆ†å‰²**: ç¢ºä¿æ¯å€‹åƒèˆ‡è€…ç²å¾—ç›¸åŒçš„é¡åˆ¥åˆ†å¸ƒ
- **æ ¼å¼çµ±ä¸€**: è¼¸å‡ºPyTorch tensoræ ¼å¼ï¼Œä¾¿æ–¼å¾ŒçºŒè™•ç†
- **å®Œæ•´æ€§æª¢æŸ¥**: é©—è­‰åˆ†ç‰‡ç¸½æ•¸ç­‰æ–¼åŸå§‹æ•¸æ“šé›†å¤§å°

### 2. çµ±ä¸€æ¨¡å‹æ¶æ§‹ (`models.py`)

#### ResNetæ¶æ§‹è¨­è¨ˆ
```python
class CIFAR10ResNet(nn.Module):
    def __init__(self, dropout_rate=0.3):
        super().__init__()
        # å·ç©å±¤çµ„åˆ
        self.conv_layers = nn.Sequential(
            # Block 1: 32x32 -> 16x16
            nn.Conv2d(3, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Dropout2d(0.15),  # æ¼¸é€²å¼dropout
            
            # Block 2: 16x16 -> 8x8  
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Dropout2d(0.2),
            
            # Block 3: 8x8 -> 4x4
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128), 
            nn.ReLU(),
            nn.Dropout2d(0.3),
        )
        
        # åˆ†é¡å±¤
        self.classifier = nn.Sequential(
            nn.Linear(128 * 4 * 4, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 10)
        )
```

#### é˜²éæ“¬åˆç­–ç•¥
1. **BatchNorm**: æ¯å€‹å·ç©å±¤å¾Œæ·»åŠ ï¼Œç©©å®šè¨“ç·´
2. **æ¼¸é€²å¼Dropout**: 0.15 â†’ 0.2 â†’ 0.3ï¼Œé€å±¤å¢å¼·
3. **L2æ­£å‰‡åŒ–**: weight_decay=1e-4
4. **å­¸ç¿’ç‡èª¿åº¦**: StepLRï¼Œæ¯5è¼ªè¡°æ¸›0.8å€

### 3. å‚³çµ±è¯é‚¦å­¸ç¿’å¯¦ç¾

#### æœå‹™å™¨ç«¯ (`server_cifar10.py`)

##### æ ¸å¿ƒè¯é‚¦å¹³å‡ç®—æ³•
```python
def federated_averaging(self, model_updates, weights):
    """
    ç²¾ç¢ºçš„è¯é‚¦å¹³å‡ç®—æ³•
    """
    # è¨ˆç®—æ­¸ä¸€åŒ–æ¬Šé‡
    total_samples = sum(weights)
    normalized_weights = [w / total_samples for w in weights]
    
    # ç²å–åƒè€ƒæ¨¡å‹ç‹€æ…‹
    global_state_dict = self.global_model.state_dict()
    
    # ç²¾ç¢ºåŠ æ¬Šå¹³å‡
    for key in global_state_dict.keys():
        # ä¿å­˜åŸå§‹æ•¸æ“šé¡å‹
        original_dtype = global_state_dict[key].dtype
        
        # åˆå§‹åŒ–ç‚ºé›¶å¼µé‡
        global_state_dict[key] = torch.zeros_like(global_state_dict[key]).float()
        
        # åŠ æ¬Šæ±‚å’Œ
        for i, update in enumerate(model_updates):
            weight = normalized_weights[i]
            param = update[key].float()
            global_state_dict[key] += weight * param
        
        # è½‰å›åŸå§‹æ•¸æ“šé¡å‹
        global_state_dict[key] = global_state_dict[key].to(original_dtype)
    
    return global_state_dict
```

##### æ•…éšœæª¢æ¸¬æ©Ÿåˆ¶
```python
def wait_for_participants(self, expected_participants, timeout=30):
    """
    ç­‰å¾…åƒèˆ‡è€…é€£æ¥ï¼Œæ”¯æ´è¶…æ™‚æª¢æ¸¬
    """
    start_time = time.time()
    participants = []
    
    while len(participants) < expected_participants:
        if time.time() - start_time > timeout:
            logging.warning(f"è¶…æ™‚ï¼åƒ…æ”¶åˆ° {len(participants)} å€‹åƒèˆ‡è€…")
            break
            
        try:
            conn, addr = self.server_socket.accept()
            participants.append((conn, addr))
            logging.info(f"åƒèˆ‡è€… {addr} å·²é€£æ¥")
        except socket.timeout:
            continue
    
    return participants
```

#### åƒèˆ‡è€…ç«¯ (`participant_cifar10.py`)

##### æœ¬åœ°è¨“ç·´å¯¦ç¾
```python
def train_local_model(self, global_state_dict, rounds):
    """
    æœ¬åœ°æ¨¡å‹è¨“ç·´ï¼Œæ”¯æ´æ•…éšœæ³¨å…¥
    """
    # è¼‰å…¥å…¨å±€æ¨¡å‹
    self.model.load_state_dict(global_state_dict)
    
    # é…ç½®å„ªåŒ–å™¨ï¼ˆL2æ­£å‰‡åŒ–ï¼‰
    optimizer = optim.Adam(
        self.model.parameters(), 
        lr=self.learning_rate,
        weight_decay=1e-4  # L2æ­£å‰‡åŒ–
    )
    
    # å­¸ç¿’ç‡èª¿åº¦å™¨
    scheduler = optim.lr_scheduler.StepLR(
        optimizer, 
        step_size=5, 
        gamma=0.8
    )
    
    # æ•…éšœæ³¨å…¥é‚è¼¯
    if self.should_fail(rounds):
        logging.info(f"åƒèˆ‡è€… {self.participant_id} åœ¨ç¬¬ {rounds} è¼ªæ•…éšœ")
        return None
    
    # æœ¬åœ°è¨“ç·´å¾ªç’°
    self.model.train()
    for epoch in range(self.local_epochs):
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            output = self.model(data)
            loss = F.cross_entropy(output, target)
            loss.backward()
            optimizer.step()
    
    scheduler.step()
    return self.model.state_dict()
```

### 4. Sparkè¯é‚¦å­¸ç¿’å¯¦ç¾ (`spark_fl_cifar10.py`)

#### RDDè¡€çµ±è¿½è¹¤å®¹éŒ¯
```python
def create_participant_rdds(self, spark_context):
    """
    å‰µå»ºåƒèˆ‡è€…RDDï¼Œæ”¯æ´è¡€çµ±è¿½è¹¤
    """
    # å‰µå»ºåƒèˆ‡è€…é…ç½®RDD
    participant_configs = [
        {"id": 1, "data_path": "../data/cifar10_train_part1.pt"},
        {"id": 2, "data_path": "../data/cifar10_train_part2.pt"}
    ]
    
    # å‰µå»ºRDDä¸¦è¨­ç½®åˆ†å€
    config_rdd = spark_context.parallelize(participant_configs, 2)
    
    # è¡€çµ±è¿½è¹¤ï¼šmapæ“ä½œæœƒè¨˜éŒ„è½‰æ›é—œä¿‚
    participant_rdd = config_rdd.map(lambda config: {
        "participant_id": config["id"],
        "data": self.load_participant_data(config["data_path"]),
        "model_state": None
    })
    
    # æŒä¹…åŒ–RDDä»¥å„ªåŒ–æ€§èƒ½
    participant_rdd.persist(StorageLevel.MEMORY_AND_DISK)
    
    return participant_rdd
```

#### åˆ†æ•£å¼è¨“ç·´å¯¦ç¾
```python
def distributed_training_round(self, participant_rdd, global_model_broadcast, round_num):
    """
    åˆ†æ•£å¼è¨“ç·´è¼ªæ¬¡ï¼Œæ”¯æ´æ•…éšœæ¢å¾©
    """
    def train_participant(participant_data):
        """
        å–®å€‹åƒèˆ‡è€…çš„è¨“ç·´å‡½æ•¸
        """
        try:
            # æ•…éšœæ³¨å…¥é‚è¼¯
            if should_simulate_failure(participant_data["participant_id"], round_num, self.experiment_mode):
                logging.info(f"æ¨¡æ“¬åƒèˆ‡è€… {participant_data['participant_id']} åœ¨ç¬¬ {round_num} è¼ªæ•…éšœ")
                return None
            
            # æœ¬åœ°è¨“ç·´
            model = self.create_model()
            model.load_state_dict(global_model_broadcast.value)
            
            # è¨“ç·´é…ç½®
            optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)
            
            # è¨“ç·´å¾ªç’°
            model.train()
            for epoch in range(3):  # æœ¬åœ°è¨“ç·´è¼ªæ•¸
                for batch_idx, (data, target) in enumerate(train_loader):
                    optimizer.zero_grad()
                    output = model(data)
                    loss = F.cross_entropy(output, target)
                    loss.backward()
                    optimizer.step()
            
            scheduler.step()
            
            return {
                "participant_id": participant_data["participant_id"],
                "model_update": model.state_dict(),
                "num_samples": len(participant_data["data"][0])
            }
            
        except Exception as e:
            logging.error(f"åƒèˆ‡è€…è¨“ç·´å¤±æ•—: {e}")
            return None
    
    # åˆ†æ•£å¼åŸ·è¡Œè¨“ç·´
    training_results = participant_rdd.map(train_participant).collect()
    
    # éæ¿¾å¤±æ•—çš„åƒèˆ‡è€…
    successful_results = [r for r in training_results if r is not None]
    
    return successful_results
```

#### ç²¾ç¢ºè¯é‚¦å¹³å‡ç®—æ³•
```python
def precise_federated_averaging(self, model_updates, weights, reference_model):
    """
    ç²¾ç¢ºçš„è¯é‚¦å¹³å‡ç®—æ³•ï¼Œèˆ‡å‚³çµ±FLä¿æŒä¸€è‡´
    """
    if not model_updates:
        logging.warning("æ²’æœ‰æ¨¡å‹æ›´æ–°ï¼Œè·³éèšåˆ")
        return None
    
    # è¨ˆç®—æ­¸ä¸€åŒ–æ¬Šé‡
    total_samples = sum(weights)
    normalized_weights = [w / total_samples for w in weights]
    
    # ç²å–åƒè€ƒæ¨¡å‹çš„ç‹€æ…‹å­—å…¸ä½œç‚ºæ¨¡æ¿
    reference_state = reference_model.state_dict()
    global_state_dict = {}
    
    # ç²å–è¨­å‚™ä¿¡æ¯
    device = next(reference_model.parameters()).device
    
    # åˆå§‹åŒ– - ä¿æŒæ•¸æ“šé¡å‹
    for key in model_updates[0].keys():
        global_state_dict[key] = torch.zeros_like(reference_state[key]).to(device)
    
    # ç²¾ç¢ºåŠ æ¬Šå¹³å‡ - å…ˆè½‰ç‚ºfloatè¨ˆç®—å†è½‰å›åŸé¡å‹
    for i, update in enumerate(model_updates):
        weight = normalized_weights[i]
        for key in update.keys():
            # ç²å–åŸå§‹æ•¸æ“šé¡å‹
            original_dtype = reference_state[key].dtype
            
            # è½‰ç‚ºfloaté€²è¡Œè¨ˆç®—ï¼Œç¢ºä¿ç²¾åº¦
            param_float = update[key].to(device).float()
            global_state_dict[key] += weight * param_float
    
    # è½‰å›åŸå§‹æ•¸æ“šé¡å‹
    for key in global_state_dict.keys():
        original_dtype = reference_state[key].dtype
        global_state_dict[key] = global_state_dict[key].to(original_dtype)
    
    return global_state_dict
```

## ğŸ”§ é—œéµæŠ€è¡“å¯¦ç¾

### 1. æ•…éšœæ³¨å…¥æ©Ÿåˆ¶

#### å¤šç¨®æ•…éšœæ¨¡å¼
```python
def should_simulate_failure(participant_id, round_num, mode):
    """
    æ•…éšœæ³¨å…¥é‚è¼¯
    """
    if mode == "normal":
        return False
    elif mode == "exp1":
        # æ•¸æ“šåˆ†ç‰‡è²¢ç»å¤±æ•—ï¼šç¬¬5è¼ªåƒèˆ‡è€…1é›¢ç·š
        return participant_id == 1 and round_num == 5
    elif mode == "exp2":
        # Workerç¯€é»æ•…éšœï¼šç¬¬8è¼ªåƒèˆ‡è€…1é›¢ç·š
        return participant_id == 1 and round_num == 8
    return False
```

### 2. æ€§èƒ½ç›£æ§å’Œè¨˜éŒ„

#### çµ±ä¸€çµæœè¨˜éŒ„æ ¼å¼
```python
def record_round_results(self, round_num, accuracy, loss, timestamp, mode):
    """
    è¨˜éŒ„æ¯è¼ªå¯¦é©—çµæœ
    """
    results_file = f"../results/{self.fl_type}/{mode}/cifar10_{mode}_results.csv"
    
    with open(results_file, 'a') as f:
        f.write(f"{round_num},{timestamp:.2f},{accuracy:.2f},{loss:.4f},")
        
        if self.fl_type == "traditional":
            f.write(f"{self.active_participants},{self.failed_participants},{mode}\n")
        else:  # spark
            f.write(f"{self.active_partitions},{self.failed_partitions},{mode}\n")
```

### 3. è¨­å‚™ç®¡ç†å’Œè¨˜æ†¶é«”å„ªåŒ–

#### GPUè¨˜æ†¶é«”ç®¡ç†
```python
def setup_device_and_memory(self):
    """
    è¨­ç½®è¨­å‚™å’Œè¨˜æ†¶é«”ç®¡ç†
    """
    # è¨­å‚™æª¢æ¸¬
    if torch.cuda.is_available():
        self.device = torch.device("cuda")
        logging.info(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        
        # GPUè¨˜æ†¶é«”å„ªåŒ–
        torch.cuda.empty_cache()
        torch.backends.cudnn.benchmark = True
    else:
        self.device = torch.device("cpu")
        logging.info("ä½¿ç”¨CPU")
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
```

#### Sparkè¨˜æ†¶é«”é…ç½®
```python
def create_spark_session(self):
    """
    å‰µå»ºå„ªåŒ–çš„Sparkæœƒè©±
    """
    spark = SparkSession.builder \
        .appName("CIFAR10-FederatedLearning-Improved") \
        .config("spark.driver.memory", "12g") \
        .config("spark.executor.memory", "12g") \
        .config("spark.driver.maxResultSize", "4g") \
        .config("spark.sql.execution.arrow.maxRecordsPerBatch", "1000") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.kryoserializer.buffer.max", "1g") \
        .config("spark.python.worker.memory", "8g") \
        .getOrCreate()
    
    # è¨­ç½®æ—¥èªŒç´šåˆ¥
    spark.sparkContext.setLogLevel("WARN")
    
    return spark
```

## ğŸ§ª å¯¦é©—è…³æœ¬è¨­è¨ˆ

### è‡ªå‹•åŒ–å¯¦é©—æµç¨‹
```bash
#!/bin/bash
# run_exp1_experiment.sh

echo "=== CIFAR-10 å¯¦é©—1ï¼šæ•¸æ“šåˆ†ç‰‡è²¢ç»å¤±æ•— ==="

# 1. ç’°å¢ƒæ¸…ç†
pkill -f "participant_cifar10.py" 2>/dev/null || true
pkill -f "server_cifar10.py" 2>/dev/null || true
sleep 3

# 2. å‚³çµ±FLå¯¦é©—
echo "ğŸ§ª ç¬¬ä¸€éšæ®µï¼šå‚³çµ±FL exp1å¯¦é©—"
cd traditional_code
python server_cifar10.py --rounds 20 --model resnet --mode exp1 &
SERVER_PID=$!

sleep 5
python participant_cifar10.py --participant_id 1 --model resnet --rounds 20 --mode exp1 &
python participant_cifar10.py --participant_id 2 --model resnet --rounds 20 --mode exp1 &

wait $SERVER_PID

# 3. Spark FLå¯¦é©—  
echo "ğŸš€ ç¬¬äºŒéšæ®µï¼šSpark FL exp1å¯¦é©—"
cd ../spark_code
python spark_fl_cifar10.py --rounds 20 --model resnet --mode exp1

# 4. çµæœå°æ¯”
echo "ğŸ“Š çµæœå°æ¯”ï¼š"
tail -5 ../results/traditional/exp1/cifar10_exp1_results.csv
tail -5 ../results/spark/exp1/cifar10_spark_exp1_results.csv
```

## ğŸ“Š æ€§èƒ½å„ªåŒ–æŠ€è¡“

### 1. æ•¸æ“šè¼‰å…¥å„ªåŒ–
```python
def create_optimized_dataloader(self, data_path, batch_size=64):
    """
    å‰µå»ºå„ªåŒ–çš„æ•¸æ“šè¼‰å…¥å™¨
    """
    # è¼‰å…¥é è™•ç†æ•¸æ“š
    data, targets = torch.load(data_path)
    
    # å‰µå»ºæ•¸æ“šé›†
    dataset = TensorDataset(data, targets)
    
    # å„ªåŒ–çš„DataLoaderé…ç½®
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,  # å¤šé€²ç¨‹è¼‰å…¥
        pin_memory=True,  # GPUè¨˜æ†¶é«”å„ªåŒ–
        drop_last=True  # é¿å…ä¸å®Œæ•´æ‰¹æ¬¡
    )
    
    return dataloader
```

### 2. æ¨¡å‹åºåˆ—åŒ–å„ªåŒ–
```python
def serialize_model_efficiently(self, model_state_dict):
    """
    é«˜æ•ˆçš„æ¨¡å‹åºåˆ—åŒ–
    """
    # ä½¿ç”¨pickleé€²è¡Œåºåˆ—åŒ–
    serialized = pickle.dumps(model_state_dict)
    
    # å£“ç¸®ä»¥æ¸›å°‘ç¶²è·¯å‚³è¼¸
    import gzip
    compressed = gzip.compress(serialized)
    
    return compressed

def deserialize_model_efficiently(self, compressed_data):
    """
    é«˜æ•ˆçš„æ¨¡å‹ååºåˆ—åŒ–
    """
    import gzip
    serialized = gzip.decompress(compressed_data)
    model_state_dict = pickle.loads(serialized)
    
    return model_state_dict
```

## ğŸ” èª¿è©¦å’Œç›£æ§å·¥å…·

### 1. è©³ç´°æ—¥èªŒç³»çµ±
```python
def setup_logging(self, log_level=logging.INFO):
    """
    è¨­ç½®è©³ç´°çš„æ—¥èªŒç³»çµ±
    """
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'logs/{self.fl_type}_{self.mode}.log'),
            logging.StreamHandler()
        ]
    )
    
    # å‰µå»ºå°ˆç”¨logger
    self.logger = logging.getLogger(f"CIFAR10-{self.fl_type}")
```

### 2. æ€§èƒ½ç›£æ§
```python
def monitor_system_resources(self):
    """
    ç›£æ§ç³»çµ±è³‡æºä½¿ç”¨
    """
    import psutil
    
    # CPUä½¿ç”¨ç‡
    cpu_percent = psutil.cpu_percent(interval=1)
    
    # è¨˜æ†¶é«”ä½¿ç”¨
    memory = psutil.virtual_memory()
    memory_percent = memory.percent
    
    # GPUè¨˜æ†¶é«”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
        
    self.logger.info(f"è³‡æºä½¿ç”¨ - CPU: {cpu_percent}%, è¨˜æ†¶é«”: {memory_percent}%, GPUè¨˜æ†¶é«”: {gpu_memory:.2f}GB")
```

## ğŸ¯ ä»£ç¢¼è³ªé‡ä¿è­‰

### 1. éŒ¯èª¤è™•ç†æ©Ÿåˆ¶
```python
def robust_training_wrapper(self, training_function, *args, **kwargs):
    """
    å¥å£¯çš„è¨“ç·´åŒ…è£å™¨
    """
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            return training_function(*args, **kwargs)
        except Exception as e:
            retry_count += 1
            self.logger.warning(f"è¨“ç·´å¤±æ•— (å˜—è©¦ {retry_count}/{max_retries}): {e}")
            
            if retry_count < max_retries:
                time.sleep(2 ** retry_count)  # æŒ‡æ•¸é€€é¿
            else:
                self.logger.error(f"è¨“ç·´æœ€çµ‚å¤±æ•—: {e}")
                raise
```

### 2. åƒæ•¸é©—è­‰
```python
def validate_experiment_config(self, config):
    """
    é©—è­‰å¯¦é©—é…ç½®
    """
    required_fields = ["rounds", "model", "mode", "participants"]
    
    for field in required_fields:
        if field not in config:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€é…ç½®: {field}")
    
    if config["rounds"] <= 0:
        raise ValueError("è¼ªæ•¸å¿…é ˆå¤§æ–¼0")
    
    if config["model"] not in ["simple", "standard", "resnet"]:
        raise ValueError("ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹")
    
    if config["mode"] not in ["normal", "exp1", "exp2"]:
        raise ValueError("ä¸æ”¯æ´çš„å¯¦é©—æ¨¡å¼")
```

## ğŸ“ˆ æœªä¾†æ“´å±•è¨­è¨ˆ

### 1. æ’ä»¶åŒ–æ¶æ§‹
```python
class FaultTolerancePlugin:
    """
    å®¹éŒ¯æ©Ÿåˆ¶æ’ä»¶åŸºé¡
    """
    def __init__(self, name):
        self.name = name
    
    def detect_failure(self, participant_status):
        raise NotImplementedError
    
    def handle_failure(self, failed_participants):
        raise NotImplementedError
    
    def recover_from_failure(self, recovery_data):
        raise NotImplementedError

class CheckpointPlugin(FaultTolerancePlugin):
    """
    æª¢æŸ¥é»å®¹éŒ¯æ’ä»¶
    """
    def detect_failure(self, participant_status):
        # å¯¦ç¾æª¢æŸ¥é»æ•…éšœæª¢æ¸¬
        pass
    
    def handle_failure(self, failed_participants):
        # å¯¦ç¾æª¢æŸ¥é»æ¢å¾©
        pass

class RDDLineagePlugin(FaultTolerancePlugin):
    """
    RDDè¡€çµ±è¿½è¹¤å®¹éŒ¯æ’ä»¶
    """
    def detect_failure(self, participant_status):
        # å¯¦ç¾RDDæ•…éšœæª¢æ¸¬
        pass
    
    def handle_failure(self, failed_participants):
        # å¯¦ç¾RDDè¡€çµ±é‡ç®—
        pass
```

### 2. é…ç½®ç®¡ç†ç³»çµ±
```python
class ExperimentConfig:
    """
    å¯¦é©—é…ç½®ç®¡ç†
    """
    def __init__(self, config_file):
        self.config = self.load_config(config_file)
        self.validate_config()
    
    def load_config(self, config_file):
        import yaml
        with open(config_file, 'r') as f:
            return yaml.safe_load(f)
    
    def get_model_config(self):
        return self.config["model"]
    
    def get_training_config(self):
        return self.config["training"]
    
    def get_fault_tolerance_config(self):
        return self.config["fault_tolerance"]
```

é€™ä»½æŠ€è¡“æ–‡æª”è©³ç´°èªªæ˜äº†CIFAR-10è¯é‚¦å­¸ç¿’å®¹éŒ¯å¯¦é©—çš„ä»£ç¢¼è¨­è¨ˆå’Œå¯¦ç¾ç´°ç¯€ï¼Œç‚ºå¾ŒçºŒçš„ç¶­è­·ã€æ“´å±•å’Œå„ªåŒ–æä¾›äº†å®Œæ•´çš„æŠ€è¡“åƒè€ƒã€‚ 