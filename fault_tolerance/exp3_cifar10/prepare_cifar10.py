#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CIFAR-10 Data Preparation for Federated Learning
ä¸‹è¼‰CIFAR-10æ•¸æ“šé›†ä¸¦åˆ†å‰²ç‚º2å€‹åˆ†ç‰‡ï¼Œæ¯åˆ†ç‰‡30,000æ¨£æœ¬
"""

import torch
import torchvision
import torchvision.transforms as transforms
import numpy as np
import os
import logging
from torch.utils.data import DataLoader, TensorDataset

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def download_cifar10():
    """ä¸‹è¼‰CIFAR-10æ•¸æ“šé›†"""
    logging.info("æ­£åœ¨ä¸‹è¼‰CIFAR-10æ•¸æ“šé›†...")
    
    # CIFAR-10æ¨™æº–åŒ–åƒæ•¸
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    # ä¸‹è¼‰è¨“ç·´æ•¸æ“š
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data/raw', 
        train=True, 
        download=True,
        transform=transform
    )
    
    # ä¸‹è¼‰æ¸¬è©¦æ•¸æ“š
    test_dataset = torchvision.datasets.CIFAR10(
        root='./data/raw', 
        train=False, 
        download=True,
        transform=transform
    )
    
    logging.info(f"âœ… CIFAR-10æ•¸æ“šé›†ä¸‹è¼‰å®Œæˆ")
    logging.info(f"è¨“ç·´æ¨£æœ¬æ•¸: {len(train_dataset)}")
    logging.info(f"æ¸¬è©¦æ¨£æœ¬æ•¸: {len(test_dataset)}")
    
    return train_dataset, test_dataset

def create_federated_data(train_dataset, test_dataset, num_participants=2):
    """
    å‰µå»ºè¯é‚¦å­¸ç¿’æ•¸æ“šåˆ†ç‰‡
    
    Args:
        train_dataset: CIFAR-10è¨“ç·´æ•¸æ“šé›†
        test_dataset: CIFAR-10æ¸¬è©¦æ•¸æ“šé›†  
        num_participants: åƒèˆ‡è€…æ•¸é‡ï¼ˆé»˜èª2å€‹ï¼‰
    """
    logging.info(f"æ­£åœ¨å‰µå»º {num_participants} å€‹è¯é‚¦å­¸ç¿’æ•¸æ“šåˆ†ç‰‡...")
    
    # è½‰æ›ç‚ºtensoræ ¼å¼
    train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)
    
    # ç²å–æ‰€æœ‰è¨“ç·´æ•¸æ“š
    train_data, train_targets = next(iter(train_loader))
    test_data, test_targets = next(iter(test_loader))
    
    logging.info(f"è¨“ç·´æ•¸æ“šå½¢ç‹€: {train_data.shape}")
    logging.info(f"æ¸¬è©¦æ•¸æ“šå½¢ç‹€: {test_data.shape}")
    
    # æ•¸æ“šåˆ†ç‰‡ - IIDåˆ†é…
    total_samples = len(train_data)
    samples_per_participant = total_samples // num_participants
    
    logging.info(f"ç¸½è¨“ç·´æ¨£æœ¬: {total_samples}")
    logging.info(f"æ¯å€‹åƒèˆ‡è€…æ¨£æœ¬æ•¸: {samples_per_participant}")
    
    # éš¨æ©Ÿæ‰“äº‚ç´¢å¼•ä»¥ç¢ºä¿IIDåˆ†é…
    indices = np.random.permutation(total_samples)
    
    # å‰µå»ºæ•¸æ“šç›®éŒ„
    os.makedirs('./data', exist_ok=True)
    
    # åˆ†å‰²ä¸¦ä¿å­˜è¨“ç·´æ•¸æ“š
    for i in range(num_participants):
        start_idx = i * samples_per_participant
        end_idx = (i + 1) * samples_per_participant
        
        # ç²å–ç•¶å‰åƒèˆ‡è€…çš„æ•¸æ“šç´¢å¼•
        participant_indices = indices[start_idx:end_idx]
        
        # æå–å°æ‡‰çš„æ•¸æ“šå’Œæ¨™ç±¤
        participant_data = train_data[participant_indices]
        participant_targets = train_targets[participant_indices]
        
        # ä¿å­˜åˆ†ç‰‡æ•¸æ“š
        shard_file = f'./data/cifar10_train_part{i+1}.pt'
        torch.save({
            'data': participant_data,
            'targets': participant_targets
        }, shard_file)
        
        logging.info(f"âœ… åƒèˆ‡è€… {i+1} æ•¸æ“šå·²ä¿å­˜: {shard_file}")
        logging.info(f"   æ¨£æœ¬æ•¸: {len(participant_data)}")
        logging.info(f"   æ•¸æ“šå½¢ç‹€: {participant_data.shape}")
        logging.info(f"   é¡åˆ¥åˆ†å¸ƒ: {torch.bincount(participant_targets)}")
    
    # ä¿å­˜æ¸¬è©¦æ•¸æ“šï¼ˆæ‰€æœ‰åƒèˆ‡è€…å…±äº«ï¼‰
    test_file = './data/cifar10_test.pt'
    torch.save({
        'data': test_data,
        'targets': test_targets
    }, test_file)
    
    logging.info(f"âœ… æ¸¬è©¦æ•¸æ“šå·²ä¿å­˜: {test_file}")
    logging.info(f"   æ¸¬è©¦æ¨£æœ¬æ•¸: {len(test_data)}")

def verify_data_integrity():
    """é©—è­‰æ•¸æ“šå®Œæ•´æ€§"""
    logging.info("æ­£åœ¨é©—è­‰æ•¸æ“šå®Œæ•´æ€§...")
    
    # æª¢æŸ¥åˆ†ç‰‡æ–‡ä»¶
    total_samples = 0
    for i in range(1, 3):  # 2å€‹åƒèˆ‡è€…
        shard_file = f'./data/cifar10_train_part{i}.pt'
        if os.path.exists(shard_file):
            data = torch.load(shard_file)
            samples = len(data['data'])
            total_samples += samples
            logging.info(f"âœ… åˆ†ç‰‡ {i}: {samples} æ¨£æœ¬")
        else:
            logging.error(f"âŒ åˆ†ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {shard_file}")
            return False
    
    # æª¢æŸ¥æ¸¬è©¦æ–‡ä»¶
    test_file = './data/cifar10_test.pt'
    if os.path.exists(test_file):
        test_data = torch.load(test_file)
        test_samples = len(test_data['data'])
        logging.info(f"âœ… æ¸¬è©¦æ•¸æ“š: {test_samples} æ¨£æœ¬")
    else:
        logging.error(f"âŒ æ¸¬è©¦æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return False
    
    logging.info(f"âœ… æ•¸æ“šå®Œæ•´æ€§é©—è­‰é€šé")
    logging.info(f"ç¸½è¨“ç·´æ¨£æœ¬: {total_samples}")
    logging.info(f"æ¸¬è©¦æ¨£æœ¬: {test_samples}")
    
    return True

def display_sample_images():
    """é¡¯ç¤ºæ¨£æœ¬åœ–åƒä¿¡æ¯"""
    logging.info("æ­£åœ¨åˆ†ææ¨£æœ¬æ•¸æ“š...")
    
    # CIFAR-10é¡åˆ¥åç¨±
    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 
                   'dog', 'frog', 'horse', 'ship', 'truck']
    
    # åŠ è¼‰ç¬¬ä¸€å€‹åˆ†ç‰‡æŸ¥çœ‹æ•¸æ“š
    shard_file = './data/cifar10_train_part1.pt'
    if os.path.exists(shard_file):
        data = torch.load(shard_file)
        sample_data = data['data']
        sample_targets = data['targets']
        
        logging.info(f"ğŸ“Š åˆ†ç‰‡1æ•¸æ“šåˆ†æ:")
        logging.info(f"   æ•¸æ“šé¡å‹: {sample_data.dtype}")
        logging.info(f"   æ•¸æ“šç¯„åœ: [{sample_data.min():.3f}, {sample_data.max():.3f}]")
        logging.info(f"   åœ–åƒå°ºå¯¸: {sample_data.shape[1:]}") # [C, H, W]
        
        # é¡åˆ¥åˆ†å¸ƒçµ±è¨ˆ
        class_counts = torch.bincount(sample_targets)
        logging.info(f"   é¡åˆ¥åˆ†å¸ƒ:")
        for i, count in enumerate(class_counts):
            logging.info(f"     {class_names[i]}: {count} æ¨£æœ¬")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ CIFAR-10 è¯é‚¦å­¸ç¿’æ•¸æ“šæº–å‚™")
    print("=" * 50)
    
    try:
        # 1. ä¸‹è¼‰CIFAR-10æ•¸æ“šé›†
        train_dataset, test_dataset = download_cifar10()
        
        # 2. å‰µå»ºè¯é‚¦å­¸ç¿’æ•¸æ“šåˆ†ç‰‡
        create_federated_data(train_dataset, test_dataset, num_participants=2)
        
        # 3. é©—è­‰æ•¸æ“šå®Œæ•´æ€§
        if verify_data_integrity():
            logging.info("ğŸ‰ CIFAR-10æ•¸æ“šæº–å‚™å®Œæˆï¼")
        else:
            logging.error("âŒ æ•¸æ“šé©—è­‰å¤±æ•—ï¼")
            return
        
        # 4. é¡¯ç¤ºæ¨£æœ¬ä¿¡æ¯
        display_sample_images()
        
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("  - ./data/cifar10_train_part1.pt (åƒèˆ‡è€…1è¨“ç·´æ•¸æ“š)")
        print("  - ./data/cifar10_train_part2.pt (åƒèˆ‡è€…2è¨“ç·´æ•¸æ“š)")
        print("  - ./data/cifar10_test.pt (å…±äº«æ¸¬è©¦æ•¸æ“š)")
        print("  - ./data/raw/ (åŸå§‹CIFAR-10æ•¸æ“š)")
        
    except Exception as e:
        logging.error(f"âŒ æ•¸æ“šæº–å‚™å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 